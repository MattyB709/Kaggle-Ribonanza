{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "#!pip install scikit-learn\n",
    "\n",
    "import sklearn\n",
    "'''from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('.py','wb').write(src)\n",
    "import mylib'''\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = data['sequence']\n",
    "reactivity = data.iloc[:,4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"m = reactivity.mean(axis=1)\\nfor i, col in enumerate(reactivity):\\n    # using i allows for duplicate co lumns\\n    # inplace *may* not always work here, so IMO the next line is preferred\\n    # df.iloc[:, i].fillna(m, inplace=True)\\n    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\\nreactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update NaN to csv file\n",
    "'''m = reactivity.mean(axis=1)\n",
    "for i, col in enumerate(reactivity):\n",
    "    # using i allows for duplicate co lumns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\n",
    "reactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactivity = pd.read_csv('ReactValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
      "0              0.310505         0.310505         0.310505         0.310505   \n",
      "1              0.212115         0.212115         0.212115         0.212115   \n",
      "2              0.326960         0.326960         0.326960         0.326960   \n",
      "3              0.250710         0.250710         0.250710         0.250710   \n",
      "4              0.301335         0.301335         0.301335         0.301335   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611         0.194595         0.194595         0.194595         0.194595   \n",
      "335612         0.139225         0.139225         0.139225         0.139225   \n",
      "335613         0.285865         0.285865         0.285865         0.285865   \n",
      "335614         0.305650         0.305650         0.305650         0.305650   \n",
      "335615         0.262140         0.262140         0.262140         0.262140   \n",
      "\n",
      "        reactivity_0005  reactivity_0006  reactivity_0007  reactivity_0008  \\\n",
      "0              0.310505         0.310505         0.310505         0.310505   \n",
      "1              0.212115         0.212115         0.212115         0.212115   \n",
      "2              0.326960         0.326960         0.326960         0.326960   \n",
      "3              0.250710         0.250710         0.250710         0.250710   \n",
      "4              0.301335         0.301335         0.301335         0.301335   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611         0.194595         0.194595         0.194595         0.194595   \n",
      "335612         0.139225         0.139225         0.139225         0.139225   \n",
      "335613         0.285865         0.285865         0.285865         0.285865   \n",
      "335614         0.305650         0.305650         0.305650         0.305650   \n",
      "335615         0.262140         0.262140         0.262140         0.262140   \n",
      "\n",
      "        reactivity_0009  reactivity_0010  ...  reactivity_error_0196  \\\n",
      "0              0.310505         0.310505  ...               0.310505   \n",
      "1              0.212115         0.212115  ...               0.212115   \n",
      "2              0.326960         0.326960  ...               0.326960   \n",
      "3              0.250710         0.250710  ...               0.250710   \n",
      "4              0.301335         0.301335  ...               0.301335   \n",
      "...                 ...              ...  ...                    ...   \n",
      "335611         0.194595         0.194595  ...               0.194595   \n",
      "335612         0.139225         0.139225  ...               0.139225   \n",
      "335613         0.285865         0.285865  ...               0.285865   \n",
      "335614         0.305650         0.305650  ...               0.305650   \n",
      "335615         0.262140         0.262140  ...               0.262140   \n",
      "\n",
      "        reactivity_error_0197  reactivity_error_0198  reactivity_error_0199  \\\n",
      "0                    0.310505               0.310505               0.310505   \n",
      "1                    0.212115               0.212115               0.212115   \n",
      "2                    0.326960               0.326960               0.326960   \n",
      "3                    0.250710               0.250710               0.250710   \n",
      "4                    0.301335               0.301335               0.301335   \n",
      "...                       ...                    ...                    ...   \n",
      "335611               0.194595               0.194595               0.194595   \n",
      "335612               0.139225               0.139225               0.139225   \n",
      "335613               0.285865               0.285865               0.285865   \n",
      "335614               0.305650               0.305650               0.305650   \n",
      "335615               0.262140               0.262140               0.262140   \n",
      "\n",
      "        reactivity_error_0200  reactivity_error_0201  reactivity_error_0202  \\\n",
      "0                    0.310505               0.310505               0.310505   \n",
      "1                    0.212115               0.212115               0.212115   \n",
      "2                    0.326960               0.326960               0.326960   \n",
      "3                    0.250710               0.250710               0.250710   \n",
      "4                    0.301335               0.301335               0.301335   \n",
      "...                       ...                    ...                    ...   \n",
      "335611               0.194595               0.194595               0.194595   \n",
      "335612               0.139225               0.139225               0.139225   \n",
      "335613               0.285865               0.285865               0.285865   \n",
      "335614               0.305650               0.305650               0.305650   \n",
      "335615               0.262140               0.262140               0.262140   \n",
      "\n",
      "        reactivity_error_0203  reactivity_error_0204  reactivity_error_0205  \n",
      "0                    0.310505               0.310505               0.310505  \n",
      "1                    0.212115               0.212115               0.212115  \n",
      "2                    0.326960               0.326960               0.326960  \n",
      "3                    0.250710               0.250710               0.250710  \n",
      "4                    0.301335               0.301335               0.301335  \n",
      "...                       ...                    ...                    ...  \n",
      "335611               0.194595               0.194595               0.194595  \n",
      "335612               0.139225               0.139225               0.139225  \n",
      "335613               0.285865               0.285865               0.285865  \n",
      "335614               0.305650               0.305650               0.305650  \n",
      "335615               0.262140               0.262140               0.262140  \n",
      "\n",
      "[335616 rows x 411 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_13404\\1744887545.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_13404\\1744887545.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(sequences, reactivity, test_size=0.2, random_state=42)\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoding = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1]}\n",
    "    encoded_sequence = []\n",
    "    for nucleotide in sequence:\n",
    "        # Check if the nucleotide is in the encoding dictionary\n",
    "        if nucleotide in encoding:\n",
    "            encoded_sequence.append(encoding[nucleotide])\n",
    "        else:\n",
    "            # Handle unexpected characters (e.g., print a warning)\n",
    "            print(f\"Warning: Unexpected character '{nucleotide}' in sequence.\")\n",
    "            # You can choose to replace the unexpected character with a default encoding or ignore it\n",
    "\n",
    "    # Convert the list of encoded nucleotides to a PyTorch tensor\n",
    "    return torch.FloatTensor(encoded_sequence)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Encode sequences\n",
    "X_train_encoded = [encode_sequence(seq) for seq in X_train]\n",
    "X_valid_encoded = [encode_sequence(seq) for seq in X_valid]\n",
    "\n",
    "# Pad sequences to make them equal length\n",
    "X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
    "X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([268492, 206, 4])\n",
      "torch.Size([67124, 206, 4])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_padded.shape)\n",
    "print(X_valid_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention is the process of giving attention to different parts of a sequence simultaneously to find which parts of a \n",
    "# sequence affect each other.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        # the model handles the queries, keys, and values automatically\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a network for passing information forward through the model using two linear layers and a non-linear activation function\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        #2 linear and one nonlinear layer\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "#overwrites the nn.Module forward function to pass the data through the model\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows the model to understand the position of objects relative to a sequence\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #creates an empty tensor\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        #math stuff I don't need to fully understand\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        #uses a sin and cosine function with different frequencies to \"plot the points\"\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder layer takes and processes the inputs\n",
    "# Consists of a MultiHeadAttention, PositionWiseFeedForward, two normalization layers, and\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        #Creates the attention layer and feed forward network for this encoder layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        #Two normalization layers to prevent overfitting of hidden layers during processing\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # turns some input sequences to 0 to help prevent overfitting. The var dropout is the probability that this will happen, usually set between 0.2-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #Don't understand why x is inputted three times\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder passes the data from the encoder through two layers of multihead attention layers (the first one masked) \n",
    "# and then a position wise feed forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # method of passing the information forward through the normalization layers and cross and self attention layers\n",
    "        # May want to read more on the different kinds of multi-head-attention layers\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # The purpose of nn.embedding is to take discrete tokens, like words, and change them into more vectors that can be operated on. However,\n",
    "        # we already changed the sequence of DNA to orthogonal vectors, so does that mean I should just take this out? (learn more)\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        #gives information about the position to the model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        #Creates the same amount of encoder and decoder layers based on the number of layers user specifies\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "# The purpose of the mask is to hide some of the parts of the sequence from each other to prevent the \n",
    "# model from gaining information on future or irrelevant parts of the sequence\n",
    "    def generate_mask(self, src, tgt):\n",
    "        #The model creates the mask automatically, I don't need to worry about the specifics\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_vocab_size, Size of the input, in this case it should be 335616\n",
    "#tgt_vocab_size, Size of the output, in this case it should also be 335616\n",
    "#d_model, Dimensions of the hidden layers, usually 512\n",
    "#num_heads, Tutorial I saw said 8, the important thing is that d_model has to be divisble by num_heads\n",
    "#num_layers, number of hidden layers\n",
    "#d_ff, Dimensions of the feed forward layers\n",
    "#max_seq_length, The maximum length of a sequence, most likely going to be the the length of the one-hot encoded vectors. However, with the padding, they should be all the same. Maybe we don't need the padding?\n",
    "#dropout The probability of an input to get dropped during the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''criterion = torch.nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
