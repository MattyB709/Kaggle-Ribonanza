{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "#!pip install scikit-learn\n",
    "\n",
    "import sklearn\n",
    "'''from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('.py','wb').write(src)\n",
    "import mylib'''\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = data['sequence']\n",
    "reactivity = data.iloc[:,4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Python\\Romana\\Romana\\Romana.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m reactivity\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, col \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(reactivity):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# using i allows for duplicate co lumns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# inplace *may* not always work here, so IMO the next line is preferred\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# df.iloc[:, i].fillna(m, inplace=True)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     reactivity\u001b[39m.\u001b[39miloc[:, i] \u001b[39m=\u001b[39m reactivity\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mfillna(m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m reactivity\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mReactValues\u001b[39m\u001b[39m'\u001b[39m,sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1797\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2060\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2057\u001b[0m         loc \u001b[39m=\u001b[39m item_labels\u001b[39m.\u001b[39mget_loc(col)\n\u001b[0;32m   2058\u001b[0m         \u001b[39m# Go through _setitem_single_column to get\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m         \u001b[39m#  FutureWarning if relevant.\u001b[39;00m\n\u001b[1;32m-> 2060\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_column(loc, value, indexer[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2061\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   2063\u001b[0m indexer \u001b[39m=\u001b[39m maybe_convert_ix(\u001b[39m*\u001b[39mindexer)  \u001b[39m# e.g. test_setitem_frame_align\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1996\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   1994\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> 1996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_iset_item(loc, value)\n\u001b[0;32m   1998\u001b[0m \u001b[39m# We will not operate in-place, but will attempt to in the future.\u001b[39;00m\n\u001b[0;32m   1999\u001b[0m \u001b[39m#  To determine whether we need to issue a FutureWarning, see if the\u001b[39;00m\n\u001b[0;32m   2000\u001b[0m \u001b[39m#  setting in-place would work, i.e. behavior will change.\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_column_array(loc)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4155\u001b[0m, in \u001b[0;36mDataFrame._iset_item\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   4154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iset_item\u001b[39m(\u001b[39mself\u001b[39m, loc: \u001b[39mint\u001b[39m, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4155\u001b[0m     arraylike \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iset_item_mgr(loc, arraylike, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   4158\u001b[0m     \u001b[39m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   4159\u001b[0m     \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   4160\u001b[0m     \u001b[39m# value exception to occur first\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4910\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4911\u001b[0m \u001b[39melif\u001b[39;00m is_dict_like(value):\n\u001b[1;32m-> 4912\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4915\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:12016\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12012\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_reindex_for_setitem\u001b[39m(value: DataFrame \u001b[39m|\u001b[39m Series, index: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m  12013\u001b[0m     \u001b[39m# reindex if necessary\u001b[39;00m\n\u001b[0;32m  12015\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(index) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m> 12016\u001b[0m         \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m  12018\u001b[0m     \u001b[39m# GH#4107\u001b[39;00m\n\u001b[0;32m  12019\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = reactivity.mean(axis=1)\n",
    "for i, col in enumerate(reactivity):\n",
    "    # using i allows for duplicate co lumns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\n",
    "reactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_18520\\1744887545.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_18520\\1744887545.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(sequences, reactivity, test_size=0.2, random_state=42)\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoding = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1]}\n",
    "    encoded_sequence = []\n",
    "    for nucleotide in sequence:\n",
    "        # Check if the nucleotide is in the encoding dictionary\n",
    "        if nucleotide in encoding:\n",
    "            encoded_sequence.append(encoding[nucleotide])\n",
    "        else:\n",
    "            # Handle unexpected characters (e.g., print a warning)\n",
    "            print(f\"Warning: Unexpected character '{nucleotide}' in sequence.\")\n",
    "            # You can choose to replace the unexpected character with a default encoding or ignore it\n",
    "\n",
    "    # Convert the list of encoded nucleotides to a PyTorch tensor\n",
    "    return torch.FloatTensor(encoded_sequence)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Encode sequences\n",
    "X_train_encoded = [encode_sequence(seq) for seq in X_train]\n",
    "X_valid_encoded = [encode_sequence(seq) for seq in X_valid]\n",
    "\n",
    "# Pad sequences to make them equal length\n",
    "X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
    "X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
