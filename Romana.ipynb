{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "#!pip install scikit-learn\n",
    "\n",
    "import sklearn\n",
    "'''from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('.py','wb').write(src)\n",
    "import mylib'''\n",
    "import io\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = data['sequence']\n",
    "experiment = data['experiment_type']\n",
    "#reactivity = data.iloc[:,4:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sequence_id                                           sequence  \\\n",
      "0       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "1       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "2       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "3       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "4       00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
      "...              ...                                                ...   \n",
      "335611  fffee332db3a  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAGUAGAACUACC...   \n",
      "335612  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335613  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335614  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "335615  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "\n",
      "       experiment_type                                       dataset_name  \\\n",
      "0              2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
      "1              DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
      "2              2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
      "3              DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
      "4              2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
      "...                ...                                                ...   \n",
      "335611         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335612         2A3_MaP                  OpenKnot1_Twist_2A3_EternaPlayers   \n",
      "335613         DMS_MaP                  OpenKnot1_Twist_DMS_EternaPlayers   \n",
      "335614         2A3_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335615         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "\n",
      "        reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
      "0                   NaN              NaN              NaN              NaN   \n",
      "1                   NaN              NaN              NaN              NaN   \n",
      "2                   NaN              NaN              NaN              NaN   \n",
      "3                   NaN              NaN              NaN              NaN   \n",
      "4                   NaN              NaN              NaN              NaN   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611              NaN              NaN              NaN              NaN   \n",
      "335612              NaN              NaN              NaN              NaN   \n",
      "335613              NaN              NaN              NaN              NaN   \n",
      "335614              NaN              NaN              NaN              NaN   \n",
      "335615              NaN              NaN              NaN              NaN   \n",
      "\n",
      "        reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
      "0                   NaN              NaN  ...                    NaN   \n",
      "1                   NaN              NaN  ...                    NaN   \n",
      "2                   NaN              NaN  ...                    NaN   \n",
      "3                   NaN              NaN  ...                    NaN   \n",
      "4                   NaN              NaN  ...                    NaN   \n",
      "...                 ...              ...  ...                    ...   \n",
      "335611              NaN              NaN  ...                    NaN   \n",
      "335612              NaN              NaN  ...                    NaN   \n",
      "335613              NaN              NaN  ...                    NaN   \n",
      "335614              NaN              NaN  ...                    NaN   \n",
      "335615              NaN              NaN  ...                    NaN   \n",
      "\n",
      "        reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
      "0                         NaN                    NaN                    NaN  \n",
      "1                         NaN                    NaN                    NaN  \n",
      "2                         NaN                    NaN                    NaN  \n",
      "3                         NaN                    NaN                    NaN  \n",
      "4                         NaN                    NaN                    NaN  \n",
      "...                       ...                    ...                    ...  \n",
      "335611                    NaN                    NaN                    NaN  \n",
      "335612                    NaN                    NaN                    NaN  \n",
      "335613                    NaN                    NaN                    NaN  \n",
      "335614                    NaN                    NaN                    NaN  \n",
      "335615                    NaN                    NaN                    NaN  \n",
      "\n",
      "[335616 rows x 416 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dms, a3 = data[data.experiment_type=='DMS_MaP'].reset_index(drop=True), data[data.experiment_type=='2A3_MaP'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#update NaN to csv file\n",
    "m = reactivity.mean(axis=1)\n",
    "for i, col in enumerate(reactivity):\n",
    "    # using i allows for duplicate co lumns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\n",
    "reactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactivity = pd.read_csv('ReactValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_20456\\1473072840.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(sequences, reactivity, test_size=0.2, random_state=42)\n",
    "'''\n",
    "def encode_sequence(sequence):\n",
    "    encoding = {'A': 1, 'C': 2, 'G': 3, 'U': 4, '2a3_MaP':5, 'DMS_Map': 6}\n",
    "    encoded_sequence = []\n",
    "    for nucleotide in sequence:\n",
    "        # Check if the nucleotide is in the encoding dictionary\n",
    "        if nucleotide in encoding:\n",
    "            encoded_sequence.append(encoding[nucleotide])\n",
    "        else:\n",
    "            # Handle unexpected characters (e.g., print a warning)\n",
    "            print(f\"Warning: Unexpected character '{nucleotide}' in sequence.\")\n",
    "            # You can choose to replace the unexpected character with a default encoding or ignore it\n",
    "\n",
    "    # Convert the list of encoded nucleotides to a PyTorch tensor\n",
    "    return torch.FloatTensor(encoded_sequence)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Encode sequences\n",
    "X_train_encoded = [encode_sequence(seq) for seq in sequences]\n",
    "#X_valid_encoded = [encode_sequence(seq) for seq in X_valid]\n",
    "\n",
    "# Pad sequences to make them equal length\n",
    "X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
    "#X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       experiment_type  1  2  3  4  5  6  7  8  9  ... 198 199 200 201 202  \\\n",
      "0              2A3_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "1              DMS_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "2              2A3_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "3              DMS_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "4              2A3_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "...                ... .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..   \n",
      "335611         DMS_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "335612         2A3_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "335613         DMS_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "335614         2A3_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "335615         DMS_MaP  G  G  G  A  A  C  G  A  C  ...  -1  -1  -1  -1  -1   \n",
      "\n",
      "       203 204 205 206 207  \n",
      "0       -1  -1  -1  -1  -1  \n",
      "1       -1  -1  -1  -1  -1  \n",
      "2       -1  -1  -1  -1  -1  \n",
      "3       -1  -1  -1  -1  -1  \n",
      "4       -1  -1  -1  -1  -1  \n",
      "...     ..  ..  ..  ..  ..  \n",
      "335611  -1  -1  -1  -1  -1  \n",
      "335612  -1  -1  -1  -1  -1  \n",
      "335613  -1  -1  -1  -1  -1  \n",
      "335614  -1  -1  -1  -1  -1  \n",
      "335615  -1  -1  -1  -1  -1  \n",
      "\n",
      "[335616 rows x 208 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split each string into characters, creating new columns\n",
    "char_columns = sequences.str.split('', expand=True)\n",
    "\n",
    "# Drop the first empty column (resulting from splitting an empty string at the beginning)\n",
    "char_columns = char_columns.drop(columns=0)\n",
    "\n",
    "# Rename the columns if needed\n",
    "#char_columns.columns = [f'char_{i}' for i in char_columns.columns]\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df = pd.concat([experiments, char_columns], axis=1)\n",
    "df = df.fillna(-1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.sequences = x_data\n",
    "        self.reactivities = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rna_sequence = torch.tensor(self.process_input(self.sequences.iloc[index, :]).values)\n",
    "        reactivity_values = torch.tensor(self.reactivities.iloc[index, :].values)\n",
    "\n",
    "\n",
    "        return rna_sequence, reactivity_values\n",
    "\n",
    "    def process_input(self, rna_sequence):\n",
    "        # Implement your logic to process the input RNA sequence (e.g., convert to numerical representation)\n",
    "        # Return the processed input sequence\n",
    "        X_train_encoded = rna_sequence.apply(self.encode_sequence)\n",
    "        return X_train_encoded\n",
    "    \n",
    "    def encode_sequence(self, item):\n",
    "        encoding = {'A': 1, 'C': 2, 'G': 3, 'U': 4, '2A3_MaP':5, 'DMS_MaP': 6}\n",
    "        if item == '':\n",
    "             return np.nan\n",
    "        elif item == -1:\n",
    "             return -1\n",
    "        if item in encoding:\n",
    "            return encoding[item]\n",
    "        else:\n",
    "                # Handle unexpected characters (e.g., print a warning)\n",
    "                print(f\"Warning: Unexpected character '{item}' in sequence.\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df, reactivity)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 6.,  3.,  3.,  ..., -1., -1., -1.],\n",
       "         [ 5.,  3.,  3.,  ..., -1., -1., -1.],\n",
       "         [ 5.,  3.,  3.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 5.,  3.,  3.,  ..., -1., -1., -1.],\n",
       "         [ 6.,  3.,  3.,  ..., -1., -1., -1.],\n",
       "         [ 6.,  3.,  3.,  ..., -1., -1., -1.]], dtype=torch.float64),\n",
       " tensor([[0.4298, 0.4298, 0.4298,  ..., 0.4298, 0.4298, 0.4298],\n",
       "         [0.3289, 0.3289, 0.3289,  ..., 0.3289, 0.3289, 0.3289],\n",
       "         [0.3063, 0.3063, 0.3063,  ..., 0.3063, 0.3063, 0.3063],\n",
       "         ...,\n",
       "         [0.3622, 0.3622, 0.3622,  ..., 0.3622, 0.3622, 0.3622],\n",
       "         [0.3153, 0.3153, 0.3153,  ..., 0.3153, 0.3153, 0.3153],\n",
       "         [0.3456, 0.3456, 0.3456,  ..., 0.3456, 0.3456, 0.3456]],\n",
       "        dtype=torch.float64)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Python\\Romana\\Romana\\Romana.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# MultiHeadAttention is the process of giving attention to different parts of a sequence simultaneously to find which parts of a \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# sequence affect each other.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMultiHeadAttention\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_model, num_heads):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39msuper\u001b[39m(MultiHeadAttention, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "'''# MultiHeadAttention is the process of giving attention to different parts of a sequence simultaneously to find which parts of a \n",
    "# sequence affect each other.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        # the model handles the queries, keys, and values automatically\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Creates a network for passing information forward through the model using two linear layers and a non-linear activation function\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        #2 linear and one nonlinear layer\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "#overwrites the nn.Module forward function to pass the data through the model\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#allows the model to understand the position of objects relative to a sequence\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #creates an empty tensor\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        #math stuff I don't need to fully understand\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        #uses a sin and cosine function with different frequencies to \"plot the points\"\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Encoder layer takes and processes the inputs\n",
    "# Consists of a MultiHeadAttention, PositionWiseFeedForward, two normalization layers, and\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        #Creates the attention layer and feed forward network for this encoder layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        #Two normalization layers to prevent overfitting of hidden layers during processing\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # turns some input sequences to 0 to help prevent overfitting. The var dropout is the probability that this will happen, usually set between 0.2-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #Don't understand why x is inputted three times\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# The decoder passes the data from the encoder through two layers of multihead attention layers (the first one masked) \n",
    "# and then a position wise feed forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # method of passing the information forward through the normalization layers and cross and self attention layers\n",
    "        # May want to read more on the different kinds of multi-head-attention layers\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # The purpose of nn.embedding is to take discrete tokens, like words, and change them into more vectors that can be operated on. However,\n",
    "        # we already changed the sequence of DNA to orthogonal vectors, so does that mean I should just take this out? (learn more)\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        #gives information about the position to the model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        #Creates the same amount of encoder and decoder layers based on the number of layers user specifies\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "# The purpose of the mask is to hide some of the parts of the sequence from each other to prevent the \n",
    "# model from gaining information on future or irrelevant parts of the sequence\n",
    "    def generate_mask(self, src, tgt):\n",
    "        #The model creates the mask automatically, I don't need to worry about the specifics\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
