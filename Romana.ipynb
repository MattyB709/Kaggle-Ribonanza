{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = data['sequence']\n",
    "experiments = data['experiment_type']\n",
    "#reactivity = data.iloc[:,4:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sequence_id                                           sequence  \\\n",
      "0       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "1       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "2       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "3       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "4       00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
      "...              ...                                                ...   \n",
      "335611  fffee332db3a  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAGUAGAACUACC...   \n",
      "335612  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335613  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335614  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "335615  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "\n",
      "       experiment_type                                       dataset_name  \\\n",
      "0              2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
      "1              DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
      "2              2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
      "3              DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
      "4              2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
      "...                ...                                                ...   \n",
      "335611         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335612         2A3_MaP                  OpenKnot1_Twist_2A3_EternaPlayers   \n",
      "335613         DMS_MaP                  OpenKnot1_Twist_DMS_EternaPlayers   \n",
      "335614         2A3_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335615         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "\n",
      "        reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
      "0                   NaN              NaN              NaN              NaN   \n",
      "1                   NaN              NaN              NaN              NaN   \n",
      "2                   NaN              NaN              NaN              NaN   \n",
      "3                   NaN              NaN              NaN              NaN   \n",
      "4                   NaN              NaN              NaN              NaN   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611              NaN              NaN              NaN              NaN   \n",
      "335612              NaN              NaN              NaN              NaN   \n",
      "335613              NaN              NaN              NaN              NaN   \n",
      "335614              NaN              NaN              NaN              NaN   \n",
      "335615              NaN              NaN              NaN              NaN   \n",
      "\n",
      "        reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
      "0                   NaN              NaN  ...                    NaN   \n",
      "1                   NaN              NaN  ...                    NaN   \n",
      "2                   NaN              NaN  ...                    NaN   \n",
      "3                   NaN              NaN  ...                    NaN   \n",
      "4                   NaN              NaN  ...                    NaN   \n",
      "...                 ...              ...  ...                    ...   \n",
      "335611              NaN              NaN  ...                    NaN   \n",
      "335612              NaN              NaN  ...                    NaN   \n",
      "335613              NaN              NaN  ...                    NaN   \n",
      "335614              NaN              NaN  ...                    NaN   \n",
      "335615              NaN              NaN  ...                    NaN   \n",
      "\n",
      "        reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
      "0                         NaN                    NaN                    NaN  \n",
      "1                         NaN                    NaN                    NaN  \n",
      "2                         NaN                    NaN                    NaN  \n",
      "3                         NaN                    NaN                    NaN  \n",
      "4                         NaN                    NaN                    NaN  \n",
      "...                       ...                    ...                    ...  \n",
      "335611                    NaN                    NaN                    NaN  \n",
      "335612                    NaN                    NaN                    NaN  \n",
      "335613                    NaN                    NaN                    NaN  \n",
      "335614                    NaN                    NaN                    NaN  \n",
      "335615                    NaN                    NaN                    NaN  \n",
      "\n",
      "[335616 rows x 416 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dms, a3 = data[data.experiment_type=='DMS_MaP'].reset_index(drop=True), data[data.experiment_type=='2A3_MaP'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#update NaN to csv file\n",
    "m = reactivity.mean(axis=1)\n",
    "for i, col in enumerate(reactivity):\n",
    "    # using i allows for duplicate co lumns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\n",
    "reactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactivity = pd.read_csv('ReactValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split each string into characters, creating new columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m char_columns \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop the first empty column (resulting from splitting an empty string at the beginning)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m char_columns \u001b[38;5;241m=\u001b[39m char_columns\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# Split each string into characters, creating new columns\n",
    "char_columns = sequences.str.split('', expand=True)\n",
    "\n",
    "# Drop the first empty column (resulting from splitting an empty string at the beginning)\n",
    "char_columns = char_columns.drop(columns=0)\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df = pd.concat([experiments, char_columns], axis=1)\n",
    "df = df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.sequences = x_data\n",
    "        self.reactivities = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rna_sequence = torch.tensor(self.process_input(self.sequences.iloc[index, :]).values)\n",
    "        reactivity_values = torch.tensor(self.reactivities.iloc[index, :].values)\n",
    "\n",
    "\n",
    "        return rna_sequence, reactivity_values\n",
    "\n",
    "    def process_input(self, rna_sequence):\n",
    "        # Implement your logic to process the input RNA sequence (e.g., convert to numerical representation)\n",
    "        # Return the processed input sequence\n",
    "        X_train_encoded = rna_sequence.apply(self.encode_sequence)\n",
    "        return X_train_encoded\n",
    "    \n",
    "    def encode_sequence(self, item):\n",
    "        encoding = {'A': 1, 'C': 2, 'G': 3, 'U': 4, '2A3_MaP':5, 'DMS_MaP':6}\n",
    "        if item == '':\n",
    "             return -1\n",
    "        elif item == -1:\n",
    "             return -1\n",
    "        if item in encoding:\n",
    "            return encoding[item]\n",
    "        else:\n",
    "                # Handle unexpected characters (e.g., print a warning)\n",
    "                print(f\"Warning: Unexpected character '{item}' in sequence.\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df, reactivity)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a network for passing information forward through the model using two linear layers and a non-linear activation function\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        #2 linear and one nonlinear layer\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "#overwrites the nn.Module forward function to pass the data through the model\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows the model to understand the position of objects relative to a sequence\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #creates an empty tensor\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        #uses a sin and cosine function with different frequencies to \"plot the points\"\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # The purpose of nn.embedding is to take discrete tokens, like words, and change them into more vectors that can be operated on. However,\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        #gives information about the position to the model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        #Creates the amount of encoder layers that the user specifies\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "# The purpose of the mask is to hide some of the parts of the sequence from each other to prevent the \n",
    "# model from gaining information on future or irrelevant parts of the sequence\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "input_dim = 7  # A,C,G,U,DMS_MaP, 2A3_MaP, PAD\n",
    "d_model = 512 \n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 208  # Adjust as per your sequence length\n",
    "\n",
    "model = Transformer(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']].clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MAE(Metric):\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.x,self.y = [],[]\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]]\n",
    "        y = learn.y['react'][learn.y['mask']].clip(0,1)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
