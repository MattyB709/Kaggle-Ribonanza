{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Python\\Romana\\Romana\\Romana.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Python/Romana/Romana/Romana.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#!pip install scikit-learn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "#!wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "#!pip install scikit-learn\n",
    "\n",
    "import sklearn\n",
    "'''from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('.py','wb').write(src)\n",
    "import mylib'''\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
      "0                   NaN              NaN              NaN              NaN   \n",
      "1                   NaN              NaN              NaN              NaN   \n",
      "2                   NaN              NaN              NaN              NaN   \n",
      "3                   NaN              NaN              NaN              NaN   \n",
      "4                   NaN              NaN              NaN              NaN   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611              NaN              NaN              NaN              NaN   \n",
      "335612              NaN              NaN              NaN              NaN   \n",
      "335613              NaN              NaN              NaN              NaN   \n",
      "335614              NaN              NaN              NaN              NaN   \n",
      "335615              NaN              NaN              NaN              NaN   \n",
      "\n",
      "        reactivity_0005  reactivity_0006  reactivity_0007  reactivity_0008  \\\n",
      "0                   NaN              NaN              NaN              NaN   \n",
      "1                   NaN              NaN              NaN              NaN   \n",
      "2                   NaN              NaN              NaN              NaN   \n",
      "3                   NaN              NaN              NaN              NaN   \n",
      "4                   NaN              NaN              NaN              NaN   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611              NaN              NaN              NaN              NaN   \n",
      "335612              NaN              NaN              NaN              NaN   \n",
      "335613              NaN              NaN              NaN              NaN   \n",
      "335614              NaN              NaN              NaN              NaN   \n",
      "335615              NaN              NaN              NaN              NaN   \n",
      "\n",
      "        reactivity_0009  reactivity_0010  ...  reactivity_error_0196  \\\n",
      "0                   NaN              NaN  ...                    NaN   \n",
      "1                   NaN              NaN  ...                    NaN   \n",
      "2                   NaN              NaN  ...                    NaN   \n",
      "3                   NaN              NaN  ...                    NaN   \n",
      "4                   NaN              NaN  ...                    NaN   \n",
      "...                 ...              ...  ...                    ...   \n",
      "335611              NaN              NaN  ...                    NaN   \n",
      "335612              NaN              NaN  ...                    NaN   \n",
      "335613              NaN              NaN  ...                    NaN   \n",
      "335614              NaN              NaN  ...                    NaN   \n",
      "335615              NaN              NaN  ...                    NaN   \n",
      "\n",
      "        reactivity_error_0197  reactivity_error_0198  reactivity_error_0199  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0200  reactivity_error_0201  reactivity_error_0202  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0203  reactivity_error_0204  reactivity_error_0205  \n",
      "0                         NaN                    NaN                    NaN  \n",
      "1                         NaN                    NaN                    NaN  \n",
      "2                         NaN                    NaN                    NaN  \n",
      "3                         NaN                    NaN                    NaN  \n",
      "4                         NaN                    NaN                    NaN  \n",
      "...                       ...                    ...                    ...  \n",
      "335611                    NaN                    NaN                    NaN  \n",
      "335612                    NaN                    NaN                    NaN  \n",
      "335613                    NaN                    NaN                    NaN  \n",
      "335614                    NaN                    NaN                    NaN  \n",
      "335615                    NaN                    NaN                    NaN  \n",
      "\n",
      "[335616 rows x 411 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train_data_QUICK_START.csv')\n",
    "sequences = data['sequence']\n",
    "reactivity = data.iloc[:,4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sequence_id                                           sequence  \\\n",
      "0       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "1       0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
      "2       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "3       0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
      "4       00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
      "...              ...                                                ...   \n",
      "335611  fffee332db3a  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAGUAGAACUACC...   \n",
      "335612  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335613  ffff7786573c  GGGAACGACUCGAGUAGAGUCGAAAAGAAGACGUGACGAAAAGUCA...   \n",
      "335614  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "335615  ffffa291ee37  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGAAUGCGAAGAACC...   \n",
      "\n",
      "       experiment_type                                       dataset_name  \\\n",
      "0              2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
      "1              DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
      "2              2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
      "3              DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
      "4              2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
      "...                ...                                                ...   \n",
      "335611         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335612         2A3_MaP                  OpenKnot1_Twist_2A3_EternaPlayers   \n",
      "335613         DMS_MaP                  OpenKnot1_Twist_DMS_EternaPlayers   \n",
      "335614         2A3_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "335615         DMS_MaP  DasLabBigLib_OneMil_RNAmake_designs_delete_lon...   \n",
      "\n",
      "        reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
      "0                   NaN              NaN              NaN              NaN   \n",
      "1                   NaN              NaN              NaN              NaN   \n",
      "2                   NaN              NaN              NaN              NaN   \n",
      "3                   NaN              NaN              NaN              NaN   \n",
      "4                   NaN              NaN              NaN              NaN   \n",
      "...                 ...              ...              ...              ...   \n",
      "335611              NaN              NaN              NaN              NaN   \n",
      "335612              NaN              NaN              NaN              NaN   \n",
      "335613              NaN              NaN              NaN              NaN   \n",
      "335614              NaN              NaN              NaN              NaN   \n",
      "335615              NaN              NaN              NaN              NaN   \n",
      "\n",
      "        reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
      "0                   NaN              NaN  ...                    NaN   \n",
      "1                   NaN              NaN  ...                    NaN   \n",
      "2                   NaN              NaN  ...                    NaN   \n",
      "3                   NaN              NaN  ...                    NaN   \n",
      "4                   NaN              NaN  ...                    NaN   \n",
      "...                 ...              ...  ...                    ...   \n",
      "335611              NaN              NaN  ...                    NaN   \n",
      "335612              NaN              NaN  ...                    NaN   \n",
      "335613              NaN              NaN  ...                    NaN   \n",
      "335614              NaN              NaN  ...                    NaN   \n",
      "335615              NaN              NaN  ...                    NaN   \n",
      "\n",
      "        reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "335611                    NaN                    NaN                    NaN   \n",
      "335612                    NaN                    NaN                    NaN   \n",
      "335613                    NaN                    NaN                    NaN   \n",
      "335614                    NaN                    NaN                    NaN   \n",
      "335615                    NaN                    NaN                    NaN   \n",
      "\n",
      "        reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
      "0                         NaN                    NaN                    NaN  \n",
      "1                         NaN                    NaN                    NaN  \n",
      "2                         NaN                    NaN                    NaN  \n",
      "3                         NaN                    NaN                    NaN  \n",
      "4                         NaN                    NaN                    NaN  \n",
      "...                       ...                    ...                    ...  \n",
      "335611                    NaN                    NaN                    NaN  \n",
      "335612                    NaN                    NaN                    NaN  \n",
      "335613                    NaN                    NaN                    NaN  \n",
      "335614                    NaN                    NaN                    NaN  \n",
      "335615                    NaN                    NaN                    NaN  \n",
      "\n",
      "[335616 rows x 416 columns]\n"
     ]
    }
   ],
   "source": [
    "m = reactivity.mean(axis=1)\n",
    "for i, col in enumerate(reactivity):\n",
    "    # using i allows for duplicate co lumns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    reactivity.iloc[:, i] = reactivity.iloc[:, i].fillna(m)\n",
    "reactivity.to_csv('ReactValues',sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_8796\\1744887545.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_8796\\1744887545.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(sequences, reactivity, test_size=0.2, random_state=42)\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoding = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1]}\n",
    "    encoded_sequence = []\n",
    "    for nucleotide in sequence:\n",
    "        # Check if the nucleotide is in the encoding dictionary\n",
    "        if nucleotide in encoding:\n",
    "            encoded_sequence.append(encoding[nucleotide])\n",
    "        else:\n",
    "            # Handle unexpected characters (e.g., print a warning)\n",
    "            print(f\"Warning: Unexpected character '{nucleotide}' in sequence.\")\n",
    "            # You can choose to replace the unexpected character with a default encoding or ignore it\n",
    "\n",
    "    # Convert the list of encoded nucleotides to a PyTorch tensor\n",
    "    return torch.FloatTensor(encoded_sequence)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Encode sequences\n",
    "X_train_encoded = [encode_sequence(seq) for seq in X_train]\n",
    "X_valid_encoded = [encode_sequence(seq) for seq in X_valid]\n",
    "\n",
    "# Pad sequences to make them equal length\n",
    "X_train_padded = torch.tensor(pad_sequence(X_train_encoded, batch_first=True))\n",
    "X_valid_padded = torch.tensor(pad_sequence(X_valid_encoded, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
